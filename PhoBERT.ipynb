{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features extraction using PhoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nam\\miniconda3\\envs\\torch-coding\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bert():\n",
    "    phobert = AutoModel.from_pretrained('vinai/phobert-base')\n",
    "    phobert_tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base', use_fast=False)\n",
    "    return phobert, phobert_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(s):\n",
    "    s = re.sub(r'(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})', '', s)\n",
    "    s = re.sub(r'[^\\w\\s]', '', s)\n",
    "    s = re.sub(r'\\d', '', s)\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords/vietnamese-stopwords.txt', encoding='utf-8') as f:\n",
    "    stopwords = f.readlines()\n",
    "    stopwords = [word.rstrip() for word in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thủ tướng Abe cúi đầu xin lỗi vì hành động phi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thủ tướng Nhật cúi đầu xin lỗi vì tinh thần ph...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Choáng! Cơ trưởng đeo khăn quàng quẩy banh nóc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chưa bao giờ nhạc Kpop lại dễ hát đến thế!!!\\r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Đại học Hutech sẽ áp dụng cải cách \"Tiếq Việt\"...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>“Siêu máy bay” A350 sẽ chở CĐV Việt Nam đi Mal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>Thưởng 20.000 USD cho đội tuyển cờ vua Việt Na...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>Trường Sơn giành HCV tại giải cờ vua đồng đội ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>Chuyện về chàng sinh viên Luật - Kiện tướng Lê...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>Tiền đạo Malaysia: “Tôi đã có cách vượt qua hà...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>223 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0    Thủ tướng Abe cúi đầu xin lỗi vì hành động phi...      1\n",
       "1    Thủ tướng Nhật cúi đầu xin lỗi vì tinh thần ph...      1\n",
       "2    Choáng! Cơ trưởng đeo khăn quàng quẩy banh nóc...      1\n",
       "3    Chưa bao giờ nhạc Kpop lại dễ hát đến thế!!!\\r...      1\n",
       "4    Đại học Hutech sẽ áp dụng cải cách \"Tiếq Việt\"...      1\n",
       "..                                                 ...    ...\n",
       "218  “Siêu máy bay” A350 sẽ chở CĐV Việt Nam đi Mal...      0\n",
       "219  Thưởng 20.000 USD cho đội tuyển cờ vua Việt Na...      0\n",
       "220  Trường Sơn giành HCV tại giải cờ vua đồng đội ...      0\n",
       "221  Chuyện về chàng sinh viên Luật - Kiện tướng Lê...      0\n",
       "222  Tiền đạo Malaysia: “Tôi đã có cách vượt qua hà...      0\n",
       "\n",
       "[223 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/vn_news_223_tdlfr.csv')\n",
    "df.drop(columns=['domain'],inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = df.drop('label', axis=1)\n",
    "y_df = df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "phobert, phobert_tokenizer = load_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bert_features(data, stop_words, bert, tokenizer):\n",
    "    tokenized_list = list()\n",
    "    max_len = 200\n",
    "    for sequence in data.values:\n",
    "        tokens = underthesea.word_tokenize(sequence[0])\n",
    "        token_list = list()\n",
    "        for token in tokens:\n",
    "            token = preprocess(token)\n",
    "            if token not in stop_words and token !='':\n",
    "                token_list.append(token)\n",
    "        sequence = ' '.join(token_list)\n",
    "        sequence = underthesea.word_tokenize(sequence, format='text')\n",
    "        tokenized = tokenizer.encode(sequence[:max_len])\n",
    "        tokenized_list.append(tokenized)\n",
    "\n",
    "    padding = np.array([i + [1]*(max_len - len(i)) for i in tokenized_list])\n",
    "    print('Shape after padding: ', padding.shape)\n",
    "\n",
    "    attention_mask = np.where(padding == 1, 0, 1)\n",
    "    print('Attention mask shape: ', attention_mask.shape)\n",
    "\n",
    "    padded = torch.tensor(padding).to(torch.long)\n",
    "    print('Pad: ', padded.shape)\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = bert(input_ids=padded, attention_mask=attention_mask)\n",
    "    features = last_hidden_states[0][:, 0, :].numpy()\n",
    "    print(features.shape)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after padding:  (223, 200)\n",
      "Attention mask shape:  (223, 200)\n",
      "Pad:  torch.Size([223, 200])\n",
      "(223, 768)\n"
     ]
    }
   ],
   "source": [
    "features = make_bert_features(X_df, stopwords, phobert, phobert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(features,\n",
    "                                                    y_df,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=y_df,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 768)\n",
      "(178,)\n",
      "(45, 768)\n",
      "(45,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.820\n",
      "SVC(C=1, gamma=0.125, kernel='linear', random_state=42)\n",
      "best prarams: {'C': 1, 'gamma': 0.125, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "parameters = {'kernel': ('linear', 'rbf'), 'C': [1, 2, 4], 'gamma': [0.125, 0.25, 0.5, 1, 2, 4]}\n",
    "clf = GridSearchCV(SVC(random_state=42), param_grid=parameters)\n",
    "grid_search = clf.fit(x_train, y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(grid_search.best_estimator_)\n",
    "\n",
    "# best prarams\n",
    "print('best prarams:', clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8222222222222222"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_classifier = MLPClassifier(\n",
    "    hidden_layer_sizes=(1000),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    random_state=42,\n",
    "    max_iter=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=1000, max_iter=10000, random_state=42)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8666666666666667"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_classifier.score(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-coding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f32163b82a662581a3dafce2f6a41641fba0985df67d8841d2528a4e8787a7b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
